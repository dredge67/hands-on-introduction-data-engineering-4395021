[[34m2023-08-04 19:12:28,753[0m] {[34mscheduler_job.py:[0m714} INFO[0m - Starting the scheduler[0m
[[34m2023-08-04 19:12:28,754[0m] {[34mscheduler_job.py:[0m719} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-08-04 19:12:28,801[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-08-04 19:12:28,819[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 54903[0m
[[34m2023-08-04 19:12:28,824[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-08-04 19:12:28,857[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-08-04T19:12:28.914+0000] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-08-04 19:17:29,002[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-08-04 19:22:29,036[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-08-04 19:27:29,079[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-08-04 19:32:29,120[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-08-04 19:32:33,638[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-08-04T19:32:32.958313+00:00 [scheduled]>[0m
[[34m2023-08-04 19:32:33,638[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-08-04 19:32:33,638[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-08-04T19:32:32.958313+00:00 [scheduled]>[0m
[[34m2023-08-04 19:32:33,641[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-08-04T19:32:32.958313+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-08-04 19:32:33,641[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-08-04T19:32:32.958313+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-08-04 19:32:33,675[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-08-04T19:32:32.958313+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-08-04 19:32:35,418[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-08-04 19:32:36,591[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-08-04T19:32:32.958313+00:00 [queued]> on host codespaces-657b09[0m
[[34m2023-08-04 19:32:40,323[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-08-04T19:32:32.958313+00:00 exited with status success for try_number 1[0m
[[34m2023-08-04 19:32:40,330[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-08-04T19:32:32.958313+00:00, map_index=-1, run_start_date=2023-08-04 19:32:36.660266+00:00, run_end_date=2023-08-04 19:32:39.221044+00:00, run_duration=2.560778, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-08-04 19:32:33.639396+00:00, queued_by_job_id=1, pid=61784[0m
[[34m2023-08-04 19:32:40,461[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-08-04T19:32:32.958313+00:00 [scheduled]>[0m
[[34m2023-08-04 19:32:40,473[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-08-04 19:32:40,474[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-08-04T19:32:32.958313+00:00 [scheduled]>[0m
[[34m2023-08-04 19:32:40,510[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-08-04T19:32:32.958313+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-08-04 19:32:40,511[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-08-04T19:32:32.958313+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-08-04 19:32:40,544[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-08-04T19:32:32.958313+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-08-04 19:32:41,565[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-08-04 19:32:42,431[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-08-04T19:32:32.958313+00:00 [queued]> on host codespaces-657b09[0m
[[34m2023-08-04 19:32:43,487[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-08-04T19:32:32.958313+00:00 exited with status success for try_number 1[0m
[[34m2023-08-04 19:32:43,491[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-08-04T19:32:32.958313+00:00, map_index=-1, run_start_date=2023-08-04 19:32:42.499936+00:00, run_end_date=2023-08-04 19:32:42.938988+00:00, run_duration=0.439052, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-08-04 19:32:40.475249+00:00, queued_by_job_id=1, pid=61824[0m
[[34m2023-08-04 19:32:43,574[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-08-04T19:32:32.958313+00:00 [scheduled]>[0m
[[34m2023-08-04 19:32:43,574[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-08-04 19:32:43,574[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-08-04T19:32:32.958313+00:00 [scheduled]>[0m
[[34m2023-08-04 19:32:43,577[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-08-04T19:32:32.958313+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-08-04 19:32:43,578[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-08-04T19:32:32.958313+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-08-04 19:32:43,619[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-08-04T19:32:32.958313+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-08-04 19:32:44,716[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-08-04 19:32:45,542[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-08-04T19:32:32.958313+00:00 [queued]> on host codespaces-657b09[0m
[[34m2023-08-04 19:32:46,657[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-08-04T19:32:32.958313+00:00 exited with status success for try_number 1[0m
[[34m2023-08-04 19:32:46,661[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-08-04T19:32:32.958313+00:00, map_index=-1, run_start_date=2023-08-04 19:32:45.617368+00:00, run_end_date=2023-08-04 19:32:46.108173+00:00, run_duration=0.490805, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-08-04 19:32:43.575448+00:00, queued_by_job_id=1, pid=61839[0m
[[34m2023-08-04 19:32:46,742[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-08-04 19:32:32.958313+00:00: manual__2023-08-04T19:32:32.958313+00:00, state:running, queued_at: 2023-08-04 19:32:33.009898+00:00. externally triggered: True> successful[0m
[[34m2023-08-04 19:32:46,743[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-08-04 19:32:32.958313+00:00, run_id=manual__2023-08-04T19:32:32.958313+00:00, run_start_date=2023-08-04 19:32:33.535501+00:00, run_end_date=2023-08-04 19:32:46.742986+00:00, run_duration=13.207485, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-08-04 19:32:32.958313+00:00, data_interval_end=2023-08-04 19:32:32.958313+00:00, dag_hash=928c1a40cd8162f40a674cbb36a96ae3[0m
[[34m2023-08-04 19:32:46,746[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-08-04 19:37:29,156[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
